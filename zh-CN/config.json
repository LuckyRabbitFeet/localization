{
  "noInstanceSelected": "æœªé€‰æ‹©æ¨¡å‹å®ä¾‹",
  "resetToDefault": "é‡ç½®",
  "showAdvancedSettings": "æ˜¾ç¤ºé«˜çº§è®¾ç½®",
  "showAll": "å…¨éƒ¨",
  "basicSettings": "åŸºç¡€",
  "configSubtitle": "åŠ è½½æˆ–ä¿å­˜é¢„è®¾å¹¶å°è¯•æ¨¡å‹å‚æ•°è¦†ç›–",
  "inferenceParameters/title": "é¢„æµ‹å‚æ•°",
  "inferenceParameters/info": "å°è¯•å½±å“é¢„æµ‹çš„å‚æ•°ã€‚",
  "generalParameters/title": "é€šç”¨",
  "samplingParameters/title": "é‡‡æ ·",
  "basicTab": "åŸºç¡€",
  "advancedTab": "é«˜çº§",
  "advancedTab/title": "ğŸ§ª é«˜çº§é…ç½®",
  "advancedTab/expandAll": "å±•å¼€æ‰€æœ‰",
  "advancedTab/overridesTitle": "é…ç½®è¦†ç›–",
  "advancedTab/noConfigsText": "æ‚¨æ²¡æœ‰æœªä¿å­˜çš„æ›´æ”¹ - ç¼–è¾‘ä¸Šæ–¹å€¼ä»¥åœ¨æ­¤å¤„æŸ¥çœ‹è¦†ç›–ã€‚",
  "loadInstanceFirst": "åŠ è½½æ¨¡å‹ä»¥æŸ¥çœ‹å¯é…ç½®å‚æ•°",
  "noListedConfigs": "æ— å¯é…ç½®å‚æ•°",
  "generationParameters/info": "å°è¯•å½±å“æ–‡æœ¬ç”Ÿæˆçš„åŸºç¡€å‚æ•°ã€‚",
  "loadParameters/title": "åŠ è½½å‚æ•°",
  "loadParameters/description": "æ§åˆ¶æ¨¡å‹åˆå§‹åŒ–å’ŒåŠ è½½åˆ°å†…å­˜çš„æ–¹å¼çš„è®¾ç½®ã€‚",
  "loadParameters/reload": "é‡æ–°åŠ è½½ä»¥åº”ç”¨æ›´æ”¹",
  "loadParameters/reload/error": "Failed to reload the model",
  "discardChanges": "æ”¾å¼ƒæ›´æ”¹",
  "loadModelToSeeOptions": "åŠ è½½æ¨¡å‹ä»¥æŸ¥çœ‹é€‰é¡¹",
  "schematicsError.title": "The config schematics contains errors in the following fields:",
  "manifestSections": {
    "structuredOutput/title": "Structured Output",
    "speculativeDecoding/title": "Speculative Decoding",
    "sampling/title": "Sampling",
    "settings/title": "Settings",
    "toolUse/title": "Tool Use",
    "promptTemplate/title": "Prompt Template"
  },

  "llm.prediction.systemPrompt/title": "ç³»ç»Ÿæç¤º",
  "llm.prediction.systemPrompt/description": "ä½¿ç”¨æ­¤å­—æ®µå‘æ¨¡å‹æä¾›èƒŒæ™¯æŒ‡ä»¤ï¼Œå¦‚ä¸€å¥—è§„åˆ™ã€çº¦æŸæˆ–ä¸€èˆ¬è¦æ±‚ã€‚",
  "llm.prediction.systemPrompt/subTitle": "AI æŒ‡å—",
  "llm.prediction.temperature/title": "æ¸©åº¦",
  "llm.prediction.temperature/subTitle": "å¼•å…¥å¤šå°‘éšæœºæ€§ã€‚0 å°†å§‹ç»ˆäº§ç”Ÿç›¸åŒçš„ç»“æœï¼Œè€Œè¾ƒé«˜å€¼å°†å¢åŠ åˆ›é€ æ€§å’Œå˜åŒ–ã€‚",
  "llm.prediction.temperature/info": "æ¥è‡ª llama.cpp å¸®åŠ©æ–‡æ¡£ï¼š\"é»˜è®¤å€¼ä¸º <{{dynamicValue}}>ï¼Œå®ƒåœ¨éšæœºæ€§å’Œç¡®å®šæ€§ä¹‹é—´æä¾›äº†å¹³è¡¡ã€‚æç«¯æƒ…å†µä¸‹ï¼Œæ¸©åº¦ä¸º 0 ä¼šå§‹ç»ˆé€‰æ‹©æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯å…ƒï¼ˆtokenï¼‰ï¼Œå¯¼è‡´æ¯æ¬¡è¿è¡Œçš„è¾“å‡ºç›¸åŒ\"",
  "llm.prediction.llama.sampling/title": "é‡‡æ ·",
  "llm.prediction.topKSampling/title": "Top K é‡‡æ ·",
  "llm.prediction.topKSampling/subTitle": "å°†ä¸‹ä¸€ä¸ªè¯å…ƒï¼ˆtokenï¼‰é™åˆ¶ä¸ºæ¨¡å‹é¢„æµ‹çš„å‰ k ä¸ªæœ€å¯èƒ½çš„è¯å…ƒï¼ˆtokenï¼‰ã€‚ä½œç”¨ç±»ä¼¼äºæ¸©åº¦",
  "llm.prediction.topKSampling/info": "æ¥è‡ª llama.cpp å¸®åŠ©æ–‡æ¡£ï¼š\n\nTop-k é‡‡æ ·æ˜¯ä¸€ç§ä»…ä»æ¨¡å‹é¢„æµ‹çš„å‰ k ä¸ªæœ€å¯èƒ½çš„è¯å…ƒï¼ˆtokenï¼‰ä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªè¯å…ƒï¼ˆtokenï¼‰çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ã€‚\n\nå®ƒæœ‰åŠ©äºå‡å°‘ç”Ÿæˆä½æ¦‚ç‡æˆ–æ— æ„ä¹‰è¯å…ƒï¼ˆtokenï¼‰çš„é£é™©ï¼Œä½†ä¹Ÿå¯èƒ½é™åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§ã€‚\n\næ›´é«˜çš„ top-k å€¼ï¼ˆä¾‹å¦‚ï¼Œ100ï¼‰å°†è€ƒè™‘æ›´å¤šè¯å…ƒï¼ˆtokenï¼‰ï¼Œä»è€Œç”Ÿæˆæ›´å¤šæ ·åŒ–çš„æ–‡æœ¬ï¼Œè€Œè¾ƒä½çš„å€¼ï¼ˆä¾‹å¦‚ï¼Œ10ï¼‰å°†ä¸“æ³¨äºæœ€å¯èƒ½çš„è¯å…ƒï¼ˆtokenï¼‰ï¼Œç”Ÿæˆæ›´ä¿å®ˆçš„æ–‡æœ¬ã€‚\n\nâ€¢ é»˜è®¤å€¼ä¸º <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU çº¿ç¨‹",
  "llm.prediction.llama.cpuThreads/subTitle": "æ¨ç†æœŸé—´ä½¿ç”¨çš„ CPU çº¿ç¨‹æ•°",
  "llm.prediction.llama.cpuThreads/info": "è®¡ç®—æœŸé—´è¦ä½¿ç”¨çš„çº¿ç¨‹æ•°ã€‚å¢åŠ çº¿ç¨‹æ•°å¹¶ä¸æ€»æ˜¯ä¸æ›´å¥½çš„æ€§èƒ½ç›¸å…³è”ã€‚é»˜è®¤å€¼ä¸º <{{dynamicValue}}>ã€‚",
  "llm.prediction.maxPredictedTokens/title": "é™åˆ¶å“åº”é•¿åº¦",
  "llm.prediction.maxPredictedTokens/subTitle": "å¯é€‰åœ°é™åˆ¶ AI å“åº”çš„é•¿åº¦",
  "llm.prediction.maxPredictedTokens/info": "æ§åˆ¶èŠå¤©æœºå™¨äººçš„å“åº”æœ€å¤§é•¿åº¦ã€‚å¼€å¯ä»¥è®¾ç½®å“åº”çš„æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œæˆ–å…³é—­ä»¥è®©èŠå¤©æœºå™¨äººå†³å®šä½•æ—¶åœæ­¢ã€‚",
  "llm.prediction.maxPredictedTokens/inputLabel": "æœ€å¤§å“åº”é•¿åº¦ï¼ˆè¯å…ƒï¼ˆtokenï¼‰ï¼‰",
  "llm.prediction.maxPredictedTokens/wordEstimate": "çº¦ {{maxWords}} è¯",
  "llm.prediction.repeatPenalty/title": "é‡å¤æƒ©ç½š",
  "llm.prediction.repeatPenalty/subTitle": "å¤šå¤§ç¨‹åº¦ä¸Šé¿å…é‡å¤ç›¸åŒçš„è¯å…ƒï¼ˆtokenï¼‰",
  "llm.prediction.repeatPenalty/info": "æ¥è‡ª llama.cpp å¸®åŠ©æ–‡æ¡£ï¼š\"æœ‰åŠ©äºé˜²æ­¢æ¨¡å‹ç”Ÿæˆé‡å¤æˆ–å•è°ƒçš„æ–‡æœ¬ã€‚\n\næ›´é«˜çš„å€¼ï¼ˆä¾‹å¦‚ï¼Œ1.5ï¼‰å°†æ›´å¼ºçƒˆåœ°æƒ©ç½šé‡å¤ï¼Œè€Œæ›´ä½çš„å€¼ï¼ˆä¾‹å¦‚ï¼Œ0.9ï¼‰å°†æ›´ä¸ºå®½å®¹ã€‚\" â€¢ é»˜è®¤å€¼ä¸º <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "æœ€å° P é‡‡æ ·",
  "llm.prediction.minPSampling/subTitle": "è¯å…ƒï¼ˆtokenï¼‰è¢«é€‰ä¸ºè¾“å‡ºçš„æœ€ä½åŸºæœ¬æ¦‚ç‡",
  "llm.prediction.minPSampling/info": "æ¥è‡ª llama.cpp å¸®åŠ©æ–‡æ¡£ï¼š\n\nç›¸å¯¹äºæœ€å¯èƒ½è¯å…ƒï¼ˆtokenï¼‰çš„æ¦‚ç‡ï¼Œè¯å…ƒï¼ˆtokenï¼‰è¢«è§†ä¸ºè€ƒè™‘çš„æœ€ä½æ¦‚ç‡ã€‚å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…ã€‚\n\nâ€¢ é»˜è®¤å€¼ä¸º <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P é‡‡æ ·",
  "llm.prediction.topPSampling/subTitle": "å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯å…ƒï¼ˆtokenï¼‰çš„æœ€å°ç´¯ç§¯æ¦‚ç‡ã€‚ä½œç”¨ç±»ä¼¼äºæ¸©åº¦",
  "llm.prediction.topPSampling/info": "æ¥è‡ª llama.cpp å¸®åŠ©æ–‡æ¡£ï¼š\n\nTop-p é‡‡æ ·ï¼Œä¹Ÿç§°ä¸ºæ ¸å¿ƒé‡‡æ ·ï¼Œæ˜¯å¦ä¸€ç§æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œä»ç´¯ç§¯æ¦‚ç‡è‡³å°‘ä¸º p çš„è¯å…ƒï¼ˆtokenï¼‰å­é›†ä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªè¯å…ƒï¼ˆtokenï¼‰ã€‚\n\nè¿™ç§æ–¹æ³•é€šè¿‡åŒæ—¶è€ƒè™‘è¯å…ƒï¼ˆtokenï¼‰çš„æ¦‚ç‡å’Œè¦ä»ä¸­é‡‡æ ·çš„è¯å…ƒï¼ˆtokenï¼‰æ•°é‡ï¼Œåœ¨å¤šæ ·æ€§å’Œè´¨é‡ä¹‹é—´æä¾›äº†å¹³è¡¡ã€‚\n\næ›´é«˜çš„ top-p å€¼ï¼ˆä¾‹å¦‚ï¼Œ0.95ï¼‰å°†å¯¼è‡´æ›´å¤šæ ·åŒ–çš„æ–‡æœ¬ï¼Œè€Œè¾ƒä½çš„å€¼ï¼ˆä¾‹å¦‚ï¼Œ0.5ï¼‰å°†ç”Ÿæˆæ›´é›†ä¸­å’Œä¿å®ˆçš„æ–‡æœ¬ã€‚å¿…é¡»åœ¨ (0, 1] èŒƒå›´å†…ã€‚\n\nâ€¢ é»˜è®¤å€¼ä¸º <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "åœæ­¢å­—ç¬¦ä¸²",
  "llm.prediction.stopStrings/subTitle": "åº”è¯¥åœæ­¢æ¨¡å‹ç”Ÿæˆæ›´å¤šè¯å…ƒï¼ˆtokenï¼‰çš„å­—ç¬¦ä¸²",
  "llm.prediction.stopStrings/info": "é‡åˆ°ç‰¹å®šå­—ç¬¦ä¸²æ—¶å°†åœæ­¢æ¨¡å‹ç”Ÿæˆæ›´å¤šè¯å…ƒï¼ˆtokenï¼‰",
  "llm.prediction.stopStrings/placeholder": "è¾“å…¥ä¸€ä¸ªå­—ç¬¦ä¸²å¹¶æŒ‰ â",
  "llm.prediction.contextOverflowPolicy/title": "ä¸Šä¸‹æ–‡æº¢å‡º",
  "llm.prediction.contextOverflowPolicy/subTitle": "å½“å¯¹è¯è¶…å‡ºæ¨¡å‹å¤„ç†èƒ½åŠ›æ—¶ï¼Œæ¨¡å‹åº”è¯¥å¦‚ä½•è¡¨ç°",
  "llm.prediction.contextOverflowPolicy/info": "å†³å®šå½“å¯¹è¯è¶…è¿‡æ¨¡å‹çš„å·¥ä½œå†…å­˜('ä¸Šä¸‹æ–‡')å¤§å°æ—¶è¯¥æ€ä¹ˆåš",
  "llm.prediction.llama.frequencyPenalty/title": "é¢‘ç‡æƒ©ç½š",
  "llm.prediction.llama.presencePenalty/title": "å­˜åœ¨æƒ©ç½š",
  "llm.prediction.llama.tailFreeSampling/title": "å°¾éƒ¨è‡ªç”±é‡‡æ ·",
  "llm.prediction.llama.locallyTypicalSampling/title": "å±€éƒ¨å…¸å‹é‡‡æ ·",
  "llm.prediction.llama.xtcProbability/title": "XTC Sampling Probability",
  "llm.prediction.llama.xtcProbability/subTitle": "The XTC (Exclude Top Choices) sampler will only be activated with this probability per generated token. XTC sampling can boost creativity and reduce clichÃ©s",
  "llm.prediction.llama.xtcProbability/info": "XTC (Exclude Top Choices) sampling will only be activated with this probability, per generated token. XTC sampling usually boosts creativity and reduces clichÃ©s",
  "llm.prediction.llama.xtcThreshold/title": "XTC Sampling Threshold",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC (Exclude Top Choices) threshold. With a chance of `xtc-probability`, search for tokens with probabilities between `xtc-threshold` and 0.5, and removes all such tokens except the least probable one",
  "llm.prediction.llama.xtcThreshold/info": "XTC (Exclude Top Choices) threshold. With a chance of `xtc-probability`, search for tokens with probabilities between `xtc-threshold` and 0.5, and removes all such tokens except the least probable one",
  "llm.prediction.mlx.topKSampling/title": "Top K Sampling",
  "llm.prediction.mlx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.mlx.topKSampling/info": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/title": "Top K é‡‡æ ·",
  "llm.prediction.onnx.topKSampling/subTitle": "å°†ä¸‹ä¸€ä¸ªè¯å…ƒï¼ˆtokenï¼‰é™åˆ¶ä¸ºå‰ k ä¸ªæœ€å¯èƒ½çš„è¯å…ƒï¼ˆtokenï¼‰ã€‚ä½œç”¨ç±»ä¼¼äºæ¸©åº¦",
  "llm.prediction.onnx.topKSampling/info": "æ¥è‡ª ONNX æ–‡æ¡£ï¼š\n\nä¿ç•™æœ€é«˜æ¦‚ç‡è¯æ±‡è¡¨è¯å…ƒï¼ˆtokenï¼‰çš„æ•°é‡ä»¥è¿›è¡Œ top-k è¿‡æ»¤\n\nâ€¢ é»˜è®¤æƒ…å†µä¸‹æ­¤è¿‡æ»¤å™¨å…³é—­",
  "llm.prediction.onnx.repeatPenalty/title": "é‡å¤æƒ©ç½š",
  "llm.prediction.onnx.repeatPenalty/subTitle": "å¤šå¤§ç¨‹åº¦ä¸Šé¿å…é‡å¤ç›¸åŒçš„è¯å…ƒï¼ˆtokenï¼‰",
  "llm.prediction.onnx.repeatPenalty/info": "æ›´é«˜çš„å€¼é˜»æ­¢æ¨¡å‹é‡å¤è‡ªèº«",
  "llm.prediction.onnx.topPSampling/title": "Top P é‡‡æ ·",
  "llm.prediction.onnx.topPSampling/subTitle": "å¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯å…ƒï¼ˆtokenï¼‰çš„æœ€å°ç´¯ç§¯æ¦‚ç‡ã€‚ä½œç”¨ç±»ä¼¼äºæ¸©åº¦",
  "llm.prediction.onnx.topPSampling/info": "æ¥è‡ª ONNX æ–‡æ¡£ï¼š\n\nä»…ä¿ç•™ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°æˆ–è¶…è¿‡ TopP çš„æœ€å¯èƒ½è¯å…ƒï¼ˆtokenï¼‰ç”¨äºç”Ÿæˆ\n\nâ€¢ é»˜è®¤æƒ…å†µä¸‹æ­¤è¿‡æ»¤å™¨å…³é—­",
  "llm.prediction.seed/title": "ç§å­",
  "llm.prediction.structured/title": "ç»“æ„åŒ–è¾“å‡º",
  "llm.prediction.structured/info": "ç»“æ„åŒ–è¾“å‡º",
  "llm.prediction.structured/description": "é«˜çº§ï¼šæ‚¨å¯ä»¥æä¾›ä¸€ä¸ª JSON æ¨¡å¼æ¥å¼ºåˆ¶æ¨¡å‹è¾“å‡ºç‰¹å®šæ ¼å¼ã€‚é˜…è¯»[æ–‡æ¡£](https://lmstudio.ai/docs/advanced/structured-output)äº†è§£æ›´å¤šä¿¡æ¯",
  "llm.prediction.tools/title": "Tool Use",
  "llm.prediction.tools/description": "Advanced: you can provide a JSON-compliant list of tools for the model to request calls to. Read the [documentation](https://lmstudio.ai/docs/advanced/tool-use) to learn more",
  "llm.prediction.tools/serverPageDescriptionAddon": "Pass this through the request body as `tools` when using the server API",
  "llm.prediction.promptTemplate/title": "æç¤ºæ¨¡æ¿",
  "llm.prediction.promptTemplate/subTitle": "èŠå¤©ä¸­æ¶ˆæ¯å‘é€ç»™æ¨¡å‹çš„æ ¼å¼ã€‚æ›´æ”¹æ­¤è®¾ç½®å¯èƒ½ä¼šå¼•å…¥æ„å¤–è¡Œä¸º - ç¡®ä¿æ‚¨çŸ¥é“è‡ªå·±åœ¨åšä»€ä¹ˆï¼",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Draft Tokens to Generate",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "The number of tokens to generate with the draft model per main model token. Find the sweet spot of compute vs. reward",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Drafting Probability Cutoff",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continue drafting until a token's probability falls below this threshold. Higher values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Min Draft Size",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Drafts smaller than this will be ignored by the main model. Higher values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Max Draft Size",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "Max number of tokens allowed in a draft. Ceiling if all token probs are > the cutoff. Lower values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.draftModel/title": "Draft Model",
  "llm.prediction.reasoning.parsing/title": "Reasoning Section Parsing",
  "llm.prediction.reasoning.parsing/subTitle": "How to parse reasoning sections in the model's output",

  "llm.load.contextLength/title": "ä¸Šä¸‹æ–‡é•¿åº¦",
  "llm.load.contextLength/subTitle": "æ¨¡å‹å¯ä»¥ä¸€æ¬¡æ€§å…³æ³¨çš„è¯å…ƒï¼ˆtokenï¼‰æœ€å¤§æ•°é‡ã€‚è¯·å‚é˜…â€œæ¨ç†å‚æ•°â€ä¸‹çš„â€œå¯¹è¯æº¢å‡ºâ€é€‰é¡¹ä»¥è·å–æ›´å¤šç®¡ç†æ–¹å¼",
  "llm.load.contextLength/info": "æŒ‡å®šæ¨¡å‹ä¸€æ¬¡å¯ä»¥è€ƒè™‘çš„æœ€å¤§è¯å…ƒï¼ˆtokenï¼‰æ•°é‡ï¼Œå½±å“å…¶å¤„ç†è¿‡ç¨‹ä¸­ä¿ç•™çš„ä¸Šä¸‹æ–‡é‡",
  "llm.load.contextLength/warning": "è®¾ç½®è¾ƒé«˜çš„ä¸Šä¸‹æ–‡é•¿åº¦å€¼ä¼šå¯¹å†…å­˜ä½¿ç”¨äº§ç”Ÿæ˜¾è‘—å½±å“",
  "llm.load.seed/title": "ç§å­",
  "llm.load.seed/subTitle": "ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ã€‚-1 è¡¨ç¤ºéšæœº",
  "llm.load.seed/info": "éšæœºç§å­ï¼šè®¾ç½®éšæœºæ•°ç”Ÿæˆçš„ç§å­ä»¥ç¡®ä¿å¯é‡å¤çš„ç»“æœ",

  "llm.load.llama.evalBatchSize/title": "è¯„ä¼°æ‰¹å¤„ç†å¤§å°",
  "llm.load.llama.evalBatchSize/subTitle": "æ¯æ¬¡å¤„ç†çš„è¾“å…¥è¯å…ƒï¼ˆtokenï¼‰æ•°é‡ã€‚å¢åŠ æ­¤å€¼ä¼šæé«˜æ€§èƒ½ï¼Œä½†ä¼šå¢åŠ å†…å­˜ä½¿ç”¨é‡",
  "llm.load.llama.evalBatchSize/info": "è®¾ç½®è¯„ä¼°æœŸé—´ä¸€èµ·å¤„ç†çš„ç¤ºä¾‹æ•°é‡ï¼Œå½±å“é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE é¢‘ç‡åŸº",
  "llm.load.llama.ropeFrequencyBase/subTitle": "æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰çš„è‡ªå®šä¹‰åŸºé¢‘ã€‚å¢åŠ æ­¤å€¼å¯èƒ½åœ¨é«˜ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹æé«˜æ€§èƒ½",
  "llm.load.llama.ropeFrequencyBase/info": "[é«˜çº§] è°ƒæ•´æ—‹è½¬ä½ç½®ç¼–ç çš„åŸºé¢‘ï¼Œå½±å“ä½ç½®ä¿¡æ¯çš„åµŒå…¥æ–¹å¼",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE é¢‘ç‡æ¯”ä¾‹",
  "llm.load.llama.ropeFrequencyScale/subTitle": "ä¸Šä¸‹æ–‡é•¿åº¦æŒ‰æ­¤å› å­ç¼©æ”¾ï¼Œä»¥ä½¿ç”¨ RoPE æ‰©å±•æœ‰æ•ˆä¸Šä¸‹æ–‡",
  "llm.load.llama.ropeFrequencyScale/info": "[é«˜çº§] ä¿®æ”¹æ—‹è½¬ä½ç½®ç¼–ç çš„é¢‘ç‡ç¼©æ”¾ï¼Œä»¥æ§åˆ¶ä½ç½®ç¼–ç çš„ç²’åº¦",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU å¸è½½",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "ç”¨äº GPU åŠ é€Ÿçš„ç¦»æ•£æ¨¡å‹å±‚æ•°",
  "llm.load.llama.acceleration.offloadRatio/info": "è®¾ç½®å¸è½½åˆ° GPU çš„å±‚æ•°ã€‚",
  "llm.load.llama.flashAttention/title": "é—ªç”µæ³¨æ„åŠ›",
  "llm.load.llama.flashAttention/subTitle": "é™ä½æŸäº›æ¨¡å‹çš„å†…å­˜ä½¿ç”¨é‡å’Œç”Ÿæˆæ—¶é—´",
  "llm.load.llama.flashAttention/info": "åŠ é€Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°æ›´å¿«ã€æ›´é«˜æ•ˆçš„å¤„ç†",
  "llm.load.numExperts/title": "ä¸“å®¶æ•°é‡",
  "llm.load.numExperts/subTitle": "æ¨¡å‹ä¸­ä½¿ç”¨çš„ä¸“å®¶æ•°é‡",
  "llm.load.numExperts/info": "æ¨¡å‹ä¸­ä½¿ç”¨çš„ä¸“å®¶æ•°é‡",
  "llm.load.llama.keepModelInMemory/title": "ä¿æŒæ¨¡å‹åœ¨å†…å­˜ä¸­",
  "llm.load.llama.keepModelInMemory/subTitle": "å³ä½¿æ¨¡å‹å¸è½½åˆ° GPU ä¹Ÿé¢„ç•™ç³»ç»Ÿå†…å­˜ã€‚æé«˜æ€§èƒ½ä½†éœ€è¦æ›´å¤šçš„ç³»ç»Ÿ RAM",
  "llm.load.llama.keepModelInMemory/info": "é˜²æ­¢æ¨¡å‹äº¤æ¢åˆ°ç£ç›˜ï¼Œç¡®ä¿æ›´å¿«çš„è®¿é—®ï¼Œä½†ä»¥æ›´é«˜çš„ RAM ä½¿ç”¨ç‡ä¸ºä»£ä»·",
  "llm.load.llama.useFp16ForKVCache/title": "ä½¿ç”¨ FP16 ç”¨äº KV ç¼“å­˜",
  "llm.load.llama.useFp16ForKVCache/info": "é€šè¿‡ä»¥åŠç²¾åº¦ï¼ˆFP16ï¼‰å­˜å‚¨ç¼“å­˜æ¥å‡å°‘å†…å­˜ä½¿ç”¨",
  "llm.load.llama.tryMmap/title": "å°è¯• mmap()",
  "llm.load.llama.tryMmap/subTitle": "æé«˜æ¨¡å‹çš„åŠ è½½æ—¶é—´ã€‚ç¦ç”¨æ­¤åŠŸèƒ½å¯èƒ½åœ¨æ¨¡å‹å¤§äºå¯ç”¨ç³»ç»Ÿ RAM æ—¶æé«˜æ€§èƒ½",
  "llm.load.llama.tryMmap/info": "ç›´æ¥ä»ç£ç›˜åŠ è½½æ¨¡å‹æ–‡ä»¶åˆ°å†…å­˜",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU Thread Pool Size",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Number of CPU threads to allocate to the thread pool used for model computation",
  "llm.load.llama.cpuThreadPoolSize/info": "The number of CPU threads to allocate to the thread pool used for model computation. Increasing the number of threads does not always correlate with better performance. The default is <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "K Cache Quantization Type",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/title": "V Cache Quantization Type",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "âš ï¸ You must disable this value if Flash Attention is not enabled",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Can only be turned on when Flash Attention is enabled",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "âš ï¸ You must disable flash attention when using F32",
  "llm.load.mlx.kvCacheBits/title": "KV Cache Quantization",
  "llm.load.mlx.kvCacheBits/subTitle": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheBits/info": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "Context Length setting is ignored when using KV Cache Quantization",
  "llm.load.mlx.kvCacheGroupSize/title": "KV Cache Quantization: Group Size",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Group size during quantization operation for the KV cache. Higher group size reduces memory usage but may decrease quality",
  "llm.load.mlx.kvCacheGroupSize/info": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV Cache Quantization: Start quantizing when ctx crosses this length",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantization/title": "KV Cache Quantization",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantize the model's KV cache. This may result in faster generation and lower memory footprint,\nat the expense of the quality of the model output.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV cache quantization bits",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "Number of bits to quantize the KV cache to",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Group size strategy",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Accuracy",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Balanced",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "Speedy",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Advanced: Quantized 'matmul group size' configuration\n\nâ€¢ Accuracy = group size 32\nâ€¢ Balanced = group size 64\nâ€¢ Speedy = group size 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Start quantizing when ctx reaches this length",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "When the context reaches this amount of tokens,\nbegin quantizing the KV cache",

  "embedding.load.contextLength/title": "ä¸Šä¸‹æ–‡é•¿åº¦",
  "embedding.load.contextLength/subTitle": "æ¨¡å‹å¯ä»¥ä¸€æ¬¡æ€§å…³æ³¨çš„è¯å…ƒï¼ˆtokenï¼‰æœ€å¤§æ•°é‡ã€‚è¯·å‚é˜…â€œæ¨ç†å‚æ•°â€ä¸‹çš„â€œå¯¹è¯æº¢å‡ºâ€é€‰é¡¹ä»¥è·å–æ›´å¤šç®¡ç†æ–¹å¼",
  "embedding.load.contextLength/info": "æŒ‡å®šæ¨¡å‹ä¸€æ¬¡å¯ä»¥è€ƒè™‘çš„æœ€å¤§è¯å…ƒï¼ˆtokenï¼‰æ•°é‡ï¼Œå½±å“å…¶å¤„ç†è¿‡ç¨‹ä¸­ä¿ç•™çš„ä¸Šä¸‹æ–‡é‡",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE é¢‘ç‡åŸº",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰çš„è‡ªå®šä¹‰åŸºé¢‘ã€‚å¢åŠ æ­¤å€¼å¯èƒ½åœ¨é«˜ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹æé«˜æ€§èƒ½",
  "embedding.load.llama.ropeFrequencyBase/info": "[é«˜çº§] è°ƒæ•´æ—‹è½¬ä½ç½®ç¼–ç çš„åŸºé¢‘ï¼Œå½±å“ä½ç½®ä¿¡æ¯çš„åµŒå…¥æ–¹å¼",
  "embedding.load.llama.evalBatchSize/title": "è¯„ä¼°æ‰¹å¤„ç†å¤§å°",
  "embedding.load.llama.evalBatchSize/subTitle": "æ¯æ¬¡å¤„ç†çš„è¾“å…¥è¯å…ƒï¼ˆtokenï¼‰æ•°é‡ã€‚å¢åŠ æ­¤å€¼ä¼šæé«˜æ€§èƒ½ï¼Œä½†ä¼šå¢åŠ å†…å­˜ä½¿ç”¨é‡",
  "embedding.load.llama.evalBatchSize/info": "è®¾ç½®è¯„ä¼°æœŸé—´ä¸€èµ·å¤„ç†çš„è¯å…ƒï¼ˆtokenï¼‰æ•°é‡",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE é¢‘ç‡æ¯”ä¾‹",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "ä¸Šä¸‹æ–‡é•¿åº¦æŒ‰æ­¤å› å­ç¼©æ”¾ï¼Œä»¥ä½¿ç”¨ RoPE æ‰©å±•æœ‰æ•ˆä¸Šä¸‹æ–‡",
  "embedding.load.llama.ropeFrequencyScale/info": "[é«˜çº§] ä¿®æ”¹æ—‹è½¬ä½ç½®ç¼–ç çš„é¢‘ç‡ç¼©æ”¾ï¼Œä»¥æ§åˆ¶ä½ç½®ç¼–ç çš„ç²’åº¦",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU å¸è½½",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "ç”¨äº GPU åŠ é€Ÿçš„ç¦»æ•£æ¨¡å‹å±‚æ•°",
  "embedding.load.llama.acceleration.offloadRatio/info": "è®¾ç½®å¸è½½åˆ° GPU çš„å±‚æ•°ã€‚",
  "embedding.load.llama.keepModelInMemory/title": "ä¿æŒæ¨¡å‹åœ¨å†…å­˜ä¸­",
  "embedding.load.llama.keepModelInMemory/subTitle": "å³ä½¿æ¨¡å‹å¸è½½åˆ° GPU ä¹Ÿé¢„ç•™ç³»ç»Ÿå†…å­˜ã€‚æé«˜æ€§èƒ½ä½†éœ€è¦æ›´å¤šçš„ç³»ç»Ÿ RAM",
  "embedding.load.llama.keepModelInMemory/info": "é˜²æ­¢æ¨¡å‹äº¤æ¢åˆ°ç£ç›˜ï¼Œç¡®ä¿æ›´å¿«çš„è®¿é—®ï¼Œä½†ä»¥æ›´é«˜çš„ RAM ä½¿ç”¨ç‡ä¸ºä»£ä»·",
  "embedding.load.llama.tryMmap/title": "å°è¯• mmap()",
  "embedding.load.llama.tryMmap/subTitle": "æé«˜æ¨¡å‹çš„åŠ è½½æ—¶é—´ã€‚ç¦ç”¨æ­¤åŠŸèƒ½å¯èƒ½åœ¨æ¨¡å‹å¤§äºå¯ç”¨ç³»ç»Ÿ RAM æ—¶æé«˜æ€§èƒ½",
  "embedding.load.llama.tryMmap/info": "ç›´æ¥ä»ç£ç›˜åŠ è½½æ¨¡å‹æ–‡ä»¶åˆ°å†…å­˜",
  "embedding.load.seed/title": "ç§å­",
  "embedding.load.seed/subTitle": "ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ã€‚-1 è¡¨ç¤ºéšæœºç§å­",

  "embedding.load.seed/info": "éšæœºç§å­ï¼šè®¾ç½®éšæœºæ•°ç”Ÿæˆçš„ç§å­ä»¥ç¡®ä¿å¯é‡å¤çš„ç»“æœ",

  "presetTooltip": {
    "included/title": "é¢„è®¾å€¼",
    "included/description": "ä»¥ä¸‹å­—æ®µå°†ä¼šè¢«åº”ç”¨",
    "included/empty": "åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ï¼Œæ­¤é¢„è®¾æ²¡æœ‰é€‚ç”¨çš„å­—æ®µã€‚",
    "included/conflict": "æ‚¨å°†è¢«è¦æ±‚é€‰æ‹©æ˜¯å¦åº”ç”¨æ­¤å€¼",
    "separateLoad/title": "åŠ è½½æ—¶é…ç½®",
    "separateLoad/description.1": "é¢„è®¾è¿˜åŒ…å«ä»¥ä¸‹åŠ è½½æ—¶é…ç½®ã€‚åŠ è½½æ—¶é…ç½®æ˜¯å…¨æ¨¡å‹èŒƒå›´çš„ï¼Œå¹¶ä¸”éœ€è¦é‡æ–°åŠ è½½æ¨¡å‹æ‰èƒ½ç”Ÿæ•ˆã€‚æŒ‰ä½",
    "separateLoad/description.2": "åº”ç”¨åˆ°",
    "separateLoad/description.3": "ã€‚",
    "excluded/title": "å¯èƒ½ä¸é€‚ç”¨",
    "excluded/description": "ä»¥ä¸‹å­—æ®µåŒ…å«åœ¨é¢„è®¾ä¸­ï¼Œä½†åœ¨å½“å‰ä¸Šä¸‹æ–‡ä¸­ä¸é€‚ç”¨ã€‚",
    "legacy/title": "æ—§ç‰ˆé¢„è®¾",
    "legacy/description": "è¿™æ˜¯ä¸€ä¸ªæ—§ç‰ˆé¢„è®¾ã€‚å®ƒåŒ…æ‹¬ä»¥ä¸‹å­—æ®µï¼Œè¿™äº›å­—æ®µç°åœ¨è¦ä¹ˆè‡ªåŠ¨å¤„ç†ï¼Œè¦ä¹ˆä¸å†é€‚ç”¨ã€‚",
    "button/publish": "Publish to Hub",
    "button/pushUpdate": "Push Changes to Hub",
    "button/export": "Export"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<ç©º>"
    },
    "checkboxNumeric": {
      "off": "å…³é—­"
    },
    "llamaCacheQuantizationType": {
      "off": "OFF"
    },
    "mlxKvCacheBits": {
      "off": "OFF"
    },
    "stringArray": {
      "empty": "<ç©º>"
    },
    "llmPromptTemplate": {
      "type": "ç±»å‹",
      "types.jinja/label": "æ¨¡æ¿ (Jinja)",
      "jinja.bosToken/label": "å¼€å§‹è¯å…ƒ (BOS Token)",
      "jinja.eosToken/label": "ç»“æŸè¯å…ƒ (EOS Token)",
      "jinja.template/label": "æ¨¡æ¿",
      "jinja/error": "è§£æ Jinja æ¨¡æ¿å¤±è´¥: {{error}}",
      "jinja/empty": "è¯·åœ¨ä¸Šæ–¹è¾“å…¥ä¸€ä¸ª Jinja æ¨¡æ¿ã€‚",
      "jinja/unlikelyToWork": "æ‚¨æä¾›çš„ Jinja æ¨¡æ¿å¾ˆå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œï¼Œå› ä¸ºå®ƒæ²¡æœ‰å¼•ç”¨å˜é‡ \"messages\"ã€‚è¯·æ£€æŸ¥æ‚¨è¾“å…¥çš„æ¨¡æ¿æ˜¯å¦æ­£ç¡®ã€‚",
      "types.manual/label": "æ‰‹åŠ¨",
      "manual.subfield.beforeSystem/label": "ç³»ç»Ÿå‰ç¼€",
      "manual.subfield.beforeSystem/placeholder": "è¾“å…¥ç³»ç»Ÿå‰ç¼€...",
      "manual.subfield.afterSystem/label": "ç³»ç»Ÿåç¼€",
      "manual.subfield.afterSystem/placeholder": "è¾“å…¥ç³»ç»Ÿåç¼€...",
      "manual.subfield.beforeUser/label": "ç”¨æˆ·å‰ç¼€",
      "manual.subfield.beforeUser/placeholder": "è¾“å…¥ç”¨æˆ·å‰ç¼€...",
      "manual.subfield.afterUser/label": "ç”¨æˆ·åç¼€",
      "manual.subfield.afterUser/placeholder": "è¾“å…¥ç”¨æˆ·åç¼€...",
      "manual.subfield.beforeAssistant/label": "åŠ©æ‰‹å‰ç¼€",
      "manual.subfield.beforeAssistant/placeholder": "è¾“å…¥åŠ©æ‰‹å‰ç¼€...",
      "manual.subfield.afterAssistant/label": "åŠ©æ‰‹åç¼€",
      "manual.subfield.afterAssistant/placeholder": "è¾“å…¥åŠ©æ‰‹åç¼€...",
      "stopStrings/label": "é¢å¤–åœæ­¢å­—ç¬¦ä¸²",
      "stopStrings/subTitle": "é™¤äº†ç”¨æˆ·æŒ‡å®šçš„åœæ­¢å­—ç¬¦ä¸²ä¹‹å¤–ï¼Œè¿˜å°†ä½¿ç”¨ç‰¹å®šäºæ¨¡æ¿çš„åœæ­¢å­—ç¬¦ä¸²ã€‚"
    },
    "contextLength": {
      "maxValueTooltip": "è¿™æ˜¯æ¨¡å‹è®­ç»ƒæ‰€èƒ½å¤„ç†çš„æœ€å¤§è¯å…ƒï¼ˆtokenï¼‰æ•°é‡ã€‚ç‚¹å‡»ä»¥å°†ä¸Šä¸‹æ–‡è®¾ç½®ä¸ºæ­¤å€¼",
      "maxValueTextStart": "æ¨¡å‹æ”¯æŒæœ€å¤š",
      "maxValueTextEnd": "ä¸ªè¯å…ƒï¼ˆtokenï¼‰",
      "tooltipHint": "å°½ç®¡æ¨¡å‹å¯èƒ½æ”¯æŒä¸€å®šæ•°é‡çš„è¯å…ƒï¼ˆtokenï¼‰ï¼Œä½†å¦‚æœæ‚¨çš„æœºå™¨èµ„æºæ— æ³•å¤„ç†è´Ÿè½½ï¼Œæ€§èƒ½å¯èƒ½ä¼šä¸‹é™ - å¢åŠ æ­¤å€¼æ—¶è¯·è°¨æ…"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "åˆ°è¾¾é™åˆ¶æ—¶åœæ­¢",
      "stopAtLimitSub": "ä¸€æ—¦æ¨¡å‹çš„å†…å­˜æ»¡è½½å³åœæ­¢ç”Ÿæˆ",
      "truncateMiddle": "æˆªæ–­ä¸­é—´",
      "truncateMiddleSub": "ä»å¯¹è¯ä¸­é—´ç§»é™¤æ¶ˆæ¯ä»¥ä¸ºæ–°æ¶ˆæ¯è…¾å‡ºç©ºé—´ã€‚æ¨¡å‹ä»ç„¶ä¼šè®°ä½å¯¹è¯çš„å¼€å¤´",
      "rollingWindow": "æ»šåŠ¨çª—å£",
      "rollingWindowSub": "æ¨¡å‹å°†å§‹ç»ˆæ¥æ”¶æœ€è¿‘çš„å‡ æ¡æ¶ˆæ¯ï¼Œä½†å¯èƒ½ä¼šå¿˜è®°å¯¹è¯çš„å¼€å¤´"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "æœ€å¤§",
      "off": "å…³é—­"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "Evenly",
      "favorMainGpu": "Favor Main GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Read how it works",
      "placeholder": "Select a compatible draft model",
      "noCompatible": "No compatible draft models found for your current model selection",
      "stillLoading": "Identifying compatible draft models...",
      "notCompatible": "The selected draft model (<draft/>) is not compatible with the current model selection (<current/>).",
      "off": "OFF",
      "loadModelToSeeOptions": "Load model <keyboard-shortcut /> to see compatible options",
      "compatibleWithNumberOfModels": "Recommended for at least {{dynamicValue}} of your models",
      "recommendedForSomeModels": "Recommended for some models",
      "recommendedForLlamaModels": "Recommended for Llama models",
      "recommendedForQwenModels": "Recommended for Qwen models",
      "onboardingModal": {
        "introducing": "Introducing",
        "speculativeDecoding": "Speculative Decoding",
        "firstStepBody": "Inference speedup for <custom-span>llama.cpp</custom-span> and <custom-span>MLX</custom-span> models",
        "secondStepTitle": "Inference Speedup with Speculative Decoding",
        "secondStepBody": "Speculative Decoding is a technique involving the collaboration of two models:\n - A larger \"main\" model\n - A smaller \"draft\" model\n\nDuring generation, the draft model rapidly proposes tokens for the larger main model to verify. Verifying tokens is a much faster process than actually generating them, which is the source of the speed gains. **Generally, the larger the size difference between the main model and the draft model, the greater the speed-up**.\n\nTo maintain quality, the main model only accepts tokens that align with what it would have generated itself, enabling the response quality of the larger model at faster inference speeds. Both models must share the same vocabulary.",
        "draftModelRecommendationsTitle": "Draft model recommendations",
        "basedOnCurrentModels": "Based on your current models",
        "close": "Close",
        "next": "Next",
        "done": "Done"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Please load a model first <model-badge /> ",
      "errorEngineNotSupported": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine (<key/>) and reload the model to use this feature.",
      "errorEngineNotSupported/noKey": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine and reload the model to use this feature."
    },
    "llmReasoningParsing": {
      "startString/label": "Start String",
      "startString/placeholder": "Enter the start string...",
      "endString/label": "End String",
      "endString/placeholder": "Enter the end string..."
    }
  },
  "saveConflictResolution": {
    "title": "é€‰æ‹©è¦åŒ…å«åœ¨é¢„è®¾ä¸­çš„å€¼",
    "description": "æŒ‘é€‰å¹¶é€‰æ‹©è¦ä¿ç•™çš„å€¼",
    "instructions": "ç‚¹å‡»ä¸€ä¸ªå€¼ä»¥åŒ…å«å®ƒ",
    "userValues": "å…ˆå‰å€¼",
    "presetValues": "æ–°å€¼",
    "confirm": "ç¡®è®¤",
    "cancel": "å–æ¶ˆ"
  },
  "applyConflictResolution": {
    "title": "ä¿ç•™å“ªäº›å€¼ï¼Ÿ",
    "description": "æ‚¨æœ‰æœªæäº¤çš„æ›´æ”¹ä¸å³å°†åº”ç”¨çš„é¢„è®¾æœ‰é‡å ",
    "instructions": "ç‚¹å‡»ä¸€ä¸ªå€¼ä»¥ä¿ç•™å®ƒ",
    "userValues": "å½“å‰å€¼",
    "presetValues": "å³å°†åº”ç”¨çš„é¢„è®¾å€¼",
    "confirm": "ç¡®è®¤",
    "cancel": "å–æ¶ˆ"
  },
  "empty": "<ç©º>",
  "noModelSelected": "No models selected",
  "apiIdentifier.label": "API Identifier",
  "apiIdentifier.hint": "Optionally provide an identifier for this model. This will be used in API requests. Leave blank to use the default identifier.",
  "idleTTL.label": "Auto Unload If Idle (TTL)",
  "idleTTL.hint": "If set, the model will be automatically unloaded after being idle for the specified amount of time.",
  "idleTTL.mins": "mins",

  "presets": {
    "title": "é¢„è®¾",
    "commitChanges": "æäº¤æ›´æ”¹",
    "commitChanges/description": "å°†æ‚¨çš„æ›´æ”¹æäº¤ç»™é¢„è®¾ã€‚",
    "commitChanges.manual": "æ£€æµ‹åˆ°æ–°çš„å­—æ®µã€‚æ‚¨å°†èƒ½å¤Ÿé€‰æ‹©è¦åŒ…å«åœ¨é¢„è®¾ä¸­çš„æ›´æ”¹ã€‚",
    "commitChanges.manual.hold.0": "æŒ‰ä½",
    "commitChanges.manual.hold.1": "é€‰æ‹©è¦æäº¤ç»™é¢„è®¾çš„æ›´æ”¹ã€‚",
    "commitChanges.saveAll.hold.0": "æŒ‰ä½",
    "commitChanges.saveAll.hold.1": "ä¿å­˜æ‰€æœ‰æ›´æ”¹ã€‚",
    "commitChanges.saveInPreset.hold.0": "æŒ‰ä½",
    "commitChanges.saveInPreset.hold.1": "ä»…ä¿å­˜å·²ç»åŒ…å«åœ¨é¢„è®¾ä¸­çš„å­—æ®µçš„æ›´æ”¹ã€‚",
    "commitChanges/error": "æœªèƒ½å°†æ›´æ”¹æäº¤ç»™é¢„è®¾ã€‚",
    "commitChanges.manual/description": "é€‰æ‹©è¦åŒ…å«åœ¨é¢„è®¾ä¸­çš„æ›´æ”¹ã€‚",
    "saveAs": "å¦å­˜ä¸ºæ–°é¢„è®¾...",
    "presetNamePlaceholder": "ä¸ºé¢„è®¾è¾“å…¥ä¸€ä¸ªåç§°...",
    "cannotCommitChangesLegacy": "è¿™æ˜¯ä¸€ä¸ªæ—§ç‰ˆé¢„è®¾ï¼Œæ— æ³•ä¿®æ”¹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨â€œå¦å­˜ä¸ºæ–°é¢„è®¾...â€åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ã€‚",
    "cannotCommitChangesNoChanges": "æ²¡æœ‰æ›´æ”¹å¯ä»¥æäº¤ã€‚",
    "emptyNoUnsaved": "é€‰æ‹©ä¸€ä¸ªé¢„è®¾...",
    "emptyWithUnsaved": "æœªä¿å­˜çš„é¢„è®¾",
    "saveEmptyWithUnsaved": "ä¿å­˜é¢„è®¾ä¸º...",
    "saveConfirm": "ä¿å­˜",
    "saveCancel": "å–æ¶ˆ",
    "saving": "æ­£åœ¨ä¿å­˜...",
    "save/error": "æœªèƒ½ä¿å­˜é¢„è®¾ã€‚",
    "deselect": "å–æ¶ˆé€‰æ‹©é¢„è®¾",
    "deselect/error": "å–æ¶ˆé€‰æ‹©é¢„è®¾å¤±è´¥ã€‚",
    "select/error": "é€‰æ‹©é¢„è®¾å¤±è´¥ã€‚",
    "delete/error": "åˆ é™¤é¢„è®¾å¤±è´¥ã€‚",
    "discardChanges": "ä¸¢å¼ƒæœªä¿å­˜çš„æ›´æ”¹",
    "discardChanges/info": "ä¸¢å¼ƒæ‰€æœ‰æœªæäº¤çš„æ›´æ”¹å¹¶æ¢å¤é¢„è®¾è‡³åŸå§‹çŠ¶æ€",
    "newEmptyPreset": "åˆ›å»ºæ–°çš„ç©ºé¢„è®¾...",
    "importPreset": "Import",
    "contextMenuSelect": "é€‰æ‹©é¢„è®¾",
    "contextMenuDelete": "åˆ é™¤",
    "contextMenuShare": "Publish...",
    "contextMenuOpenInHub": "View on Hub",
    "contextMenuPushChanges": "Push changes to Hub",
    "contextMenuPushingChanges": "Pushing...",
    "contextMenuPushedChanges": "Changes pushed",
    "contextMenuExport": "Export File",
    "contextMenuRevealInExplorer": "Reveal in File Explorer",
    "contextMenuRevealInFinder": "Reveal in Finder",
    "share": {
      "title": "Publish Preset",
      "action": "Share your preset for others to download, like, and fork",
      "presetOwnerLabel": "Owner",
      "uploadAs": "Your preset will be created as {{name}}",
      "presetNameLabel": "Preset Name",
      "descriptionLabel": "Description (optional)",
      "loading": "Publishing...",
      "success": "Preset Successfully Pushed",
      "presetIsLive": "<preset-name /> is now live on the Hub!",
      "close": "Close",
      "confirmViewOnWeb": "View on web",
      "confirmCopy": "Copy URL",
      "confirmCopied": "Copied!",
      "pushedToHub": "Your preset was pushed to the Hub",
      "descriptionPlaceholder": "Enter a description...",
      "willBePublic": "Publishing your preset will make it public",
      "publicSubtitle": "Your preset is <custom-bold>Public</custom-bold>. Others can download and fork it on lmstudio.ai",
      "confirmShareButton": "Publish",
      "error": "Failed to publish preset",
      "createFreeAccount": "Create a free account in the Hub to publish presets"
    },
    "update": {
      "title": "Push Changes to Hub",
      "title/success": "Preset Successfully Updated",
      "subtitle": "Make changes to <custom-preset-name /> and push them to the Hub",
      "descriptionLabel": "Description",
      "descriptionPlaceholder": "Enter a description...",
      "loading": "Pushing...",
      "cancel": "Cancel",
      "createFreeAccount": "Create a free account in the Hub to publish presets",
      "error": "Failed to push update",
      "confirmUpdateButton": "Push"
    },
    "import": {
      "title": "Import a Preset from File",
      "dragPrompt": "Drag and drop preset JSON files or <custom-link>select from your computer</custom-link>",
      "remove": "Remove",
      "cancel": "Cancel",
      "importPreset_zero": "Import Preset",
      "importPreset_one": "Import Preset",
      "importPreset_other": "Import {{count}} Presets",
      "selectDialog": {
        "title": "Select Preset File (.json)",
        "button": "Import"
      },
      "error": "Failed to import preset",
      "resultsModal": {
        "titleSuccessSection_one": "Imported 1 preset successfully",
        "titleSuccessSection_other": "Imported {{count}} presets successfully",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} failed)",
        "titleFailSection_other": "({{count}} failed)",
        "titleAllFailed": "Failed to import presets",
        "importMore": "Import More",
        "close": "Done",
        "successBadge": "Success",
        "alreadyExistsBadge": "Preset already exists",
        "errorBadge": "Error",
        "invalidFileBadge": "Invalid file",
        "otherErrorBadge": "Failed to import preset",
        "errorViewDetailsButton": "View Details",
        "seeError": "See Error",
        "noName": "No preset name",
        "useInChat": "Use in Chat"
      },
      "importFromUrl": {
        "button": "Import from URL...",
        "title": "Import from URL",
        "back": "Import from File...",
        "action": "Paste the LM Studio Hub URL of the preset you want to import below",
        "invalidUrl": "Invalid URL. Please make sure you are pasting a correct LM Studio Hub URL.",
        "tip": "You can install the preset directly with the {{buttonName}} button in LM Studio Hub",
        "confirm": "Import",
        "cancel": "Cancel",
        "loading": "Importing...",
        "error": "Failed to download preset."
      }
    },
    "download": {
      "title": "Pull <preset-name /> from LM Studio Hub",
      "subtitle": "Save <custom-name /> to your presets. Doing so you will allow you to use this preset in the app",
      "button": "Pull",
      "button/loading": "Pulling...",
      "cancel": "Cancel",
      "error": "Failed to download preset."
    },
    "inclusiveness": {
      "speculativeDecoding": "Include in Preset"
    }
  },

  "flashAttentionWarning": "é—ªç”µæ³¨æ„åŠ›æ˜¯ä¸€é¡¹å®éªŒæ€§åŠŸèƒ½ï¼Œå¯èƒ½ä¼šå¯¼è‡´æŸäº›æ¨¡å‹å‡ºç°é—®é¢˜ã€‚å¦‚æœæ‚¨é‡åˆ°é—®é¢˜ï¼Œè¯·å°è¯•ç¦ç”¨å®ƒã€‚",
  "llamaKvCacheQuantizationWarning": "KV Cache Quantization is an experimental feature that may cause issues with some models. Flash Attention must be enabled for V cache quantization. If you encounter problems, reset to the default \"F16\".",

  "seedUncheckedHint": "éšæœºç§å­",
  "ropeFrequencyBaseUncheckedHint": "è‡ªåŠ¨",
  "ropeFrequencyScaleUncheckedHint": "è‡ªåŠ¨",

  "hardware": {
    "advancedGpuSettings": "Advanced GPU Settings",
    "advancedGpuSettings.info": "If you're unsure, leave these at their default values",
    "advancedGpuSettings.reset": "Reset to default",
    "environmentVariables": {
      "title": "Env. Variables",
      "description": "Active environment variables during model lifetime.",
      "key.placeholder": "Select var...",
      "value.placeholder": "Value"
    },
    "mainGpu": {
      "title": "Main GPU",
      "description": "The GPU to prioritize for model computation.",
      "placeholder": "Select main GPU..."
    },
    "splitStrategy": {
      "title": "Split Strategy",
      "description": "How to split model computation across GPUs.",
      "placeholder": "Select split strategy..."
    }
  }
}

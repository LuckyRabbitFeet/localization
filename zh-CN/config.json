{
  "noInstanceSelected": "未选择模型实例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "显示高级设置",
  "showAll": "全部",
  "basicSettings": "基础",
  "configSubtitle": "加载或保存预设并尝试模型参数覆盖",
  "inferenceParameters/title": "预测参数",
  "inferenceParameters/info": "尝试影响预测的参数。",
  "generalParameters/title": "通用",
  "samplingParameters/title": "采样",
  "basicTab": "基础",
  "advancedTab": "高级",
  "advancedTab/title": "🧪 高级配置",
  "advancedTab/expandAll": "展开所有",
  "advancedTab/overridesTitle": "配置覆盖",
  "advancedTab/noConfigsText": "您没有未保存的更改 - 编辑上方值以在此处查看覆盖。",
  "loadInstanceFirst": "加载模型以查看可配置参数",
  "noListedConfigs": "无可配置参数",
  "generationParameters/info": "尝试影响文本生成的基础参数。",
  "loadParameters/title": "加载参数",
  "loadParameters/description": "控制模型初始化和加载到内存的方式的设置。",
  "loadParameters/reload": "重新加载以应用更改",
  "loadParameters/reload/error": "Failed to reload the model",
  "discardChanges": "放弃更改",
  "loadModelToSeeOptions": "加载模型以查看选项",
  "schematicsError.title": "The config schematics contains errors in the following fields:",
  "manifestSections": {
    "structuredOutput/title": "Structured Output",
    "speculativeDecoding/title": "Speculative Decoding",
    "sampling/title": "Sampling",
    "settings/title": "Settings",
    "toolUse/title": "Tool Use",
    "promptTemplate/title": "Prompt Template"
  },

  "llm.prediction.systemPrompt/title": "系统提示",
  "llm.prediction.systemPrompt/description": "使用此字段向模型提供背景指令，如一套规则、约束或一般要求。",
  "llm.prediction.systemPrompt/subTitle": "AI 指南",
  "llm.prediction.temperature/title": "温度",
  "llm.prediction.temperature/subTitle": "引入多少随机性。0 将始终产生相同的结果，而较高值将增加创造性和变化。",
  "llm.prediction.temperature/info": "来自 llama.cpp 帮助文档：\"默认值为 <{{dynamicValue}}>，它在随机性和确定性之间提供了平衡。极端情况下，温度为 0 会始终选择最可能的下一个词元（token），导致每次运行的输出相同\"",
  "llm.prediction.llama.sampling/title": "采样",
  "llm.prediction.topKSampling/title": "Top K 采样",
  "llm.prediction.topKSampling/subTitle": "将下一个词元（token）限制为模型预测的前 k 个最可能的词元（token）。作用类似于温度",
  "llm.prediction.topKSampling/info": "来自 llama.cpp 帮助文档：\n\nTop-k 采样是一种仅从模型预测的前 k 个最可能的词元（token）中选择下一个词元（token）的文本生成方法。\n\n它有助于减少生成低概率或无意义词元（token）的风险，但也可能限制输出的多样性。\n\n更高的 top-k 值（例如，100）将考虑更多词元（token），从而生成更多样化的文本，而较低的值（例如，10）将专注于最可能的词元（token），生成更保守的文本。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 线程",
  "llm.prediction.llama.cpuThreads/subTitle": "推理期间使用的 CPU 线程数",
  "llm.prediction.llama.cpuThreads/info": "计算期间要使用的线程数。增加线程数并不总是与更好的性能相关联。默认值为 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制响应长度",
  "llm.prediction.maxPredictedTokens/subTitle": "可选地限制 AI 响应的长度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天机器人的响应最大长度。开启以设置响应的最大长度限制，或关闭以让聊天机器人决定何时停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大响应长度（词元（token））",
  "llm.prediction.maxPredictedTokens/wordEstimate": "约 {{maxWords}} 词",
  "llm.prediction.repeatPenalty/title": "重复惩罚",
  "llm.prediction.repeatPenalty/subTitle": "多大程度上避免重复相同的词元（token）",
  "llm.prediction.repeatPenalty/info": "来自 llama.cpp 帮助文档：\"有助于防止模型生成重复或单调的文本。\n\n更高的值（例如，1.5）将更强烈地惩罚重复，而更低的值（例如，0.9）将更为宽容。\" • 默认值为 <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "最小 P 采样",
  "llm.prediction.minPSampling/subTitle": "词元（token）被选为输出的最低基本概率",
  "llm.prediction.minPSampling/info": "来自 llama.cpp 帮助文档：\n\n相对于最可能词元（token）的概率，词元（token）被视为考虑的最低概率。必须在 [0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P 采样",
  "llm.prediction.topPSampling/subTitle": "可能的下一个词元（token）的最小累积概率。作用类似于温度",
  "llm.prediction.topPSampling/info": "来自 llama.cpp 帮助文档：\n\nTop-p 采样，也称为核心采样，是另一种文本生成方法，从累积概率至少为 p 的词元（token）子集中选择下一个词元（token）。\n\n这种方法通过同时考虑词元（token）的概率和要从中采样的词元（token）数量，在多样性和质量之间提供了平衡。\n\n更高的 top-p 值（例如，0.95）将导致更多样化的文本，而较低的值（例如，0.5）将生成更集中和保守的文本。必须在 (0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字符串",
  "llm.prediction.stopStrings/subTitle": "应该停止模型生成更多词元（token）的字符串",
  "llm.prediction.stopStrings/info": "遇到特定字符串时将停止模型生成更多词元（token）",
  "llm.prediction.stopStrings/placeholder": "输入一个字符串并按 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "上下文溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "当对话超出模型处理能力时，模型应该如何表现",
  "llm.prediction.contextOverflowPolicy/info": "决定当对话超过模型的工作内存('上下文')大小时该怎么做",
  "llm.prediction.llama.frequencyPenalty/title": "频率惩罚",
  "llm.prediction.llama.presencePenalty/title": "存在惩罚",
  "llm.prediction.llama.tailFreeSampling/title": "尾部自由采样",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型采样",
  "llm.prediction.llama.xtcProbability/title": "XTC Sampling Probability",
  "llm.prediction.llama.xtcProbability/subTitle": "The XTC (Exclude Top Choices) sampler will only be activated with this probability per generated token. XTC sampling can boost creativity and reduce clichés",
  "llm.prediction.llama.xtcProbability/info": "XTC (Exclude Top Choices) sampling will only be activated with this probability, per generated token. XTC sampling usually boosts creativity and reduces clichés",
  "llm.prediction.llama.xtcThreshold/title": "XTC Sampling Threshold",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC (Exclude Top Choices) threshold. With a chance of `xtc-probability`, search for tokens with probabilities between `xtc-threshold` and 0.5, and removes all such tokens except the least probable one",
  "llm.prediction.llama.xtcThreshold/info": "XTC (Exclude Top Choices) threshold. With a chance of `xtc-probability`, search for tokens with probabilities between `xtc-threshold` and 0.5, and removes all such tokens except the least probable one",
  "llm.prediction.mlx.topKSampling/title": "Top K Sampling",
  "llm.prediction.mlx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.mlx.topKSampling/info": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/title": "Top K 采样",
  "llm.prediction.onnx.topKSampling/subTitle": "将下一个词元（token）限制为前 k 个最可能的词元（token）。作用类似于温度",
  "llm.prediction.onnx.topKSampling/info": "来自 ONNX 文档：\n\n保留最高概率词汇表词元（token）的数量以进行 top-k 过滤\n\n• 默认情况下此过滤器关闭",
  "llm.prediction.onnx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.onnx.repeatPenalty/subTitle": "多大程度上避免重复相同的词元（token）",
  "llm.prediction.onnx.repeatPenalty/info": "更高的值阻止模型重复自身",
  "llm.prediction.onnx.topPSampling/title": "Top P 采样",
  "llm.prediction.onnx.topPSampling/subTitle": "可能的下一个词元（token）的最小累积概率。作用类似于温度",
  "llm.prediction.onnx.topPSampling/info": "来自 ONNX 文档：\n\n仅保留累积概率达到或超过 TopP 的最可能词元（token）用于生成\n\n• 默认情况下此过滤器关闭",
  "llm.prediction.seed/title": "种子",
  "llm.prediction.structured/title": "结构化输出",
  "llm.prediction.structured/info": "结构化输出",
  "llm.prediction.structured/description": "高级：您可以提供一个 JSON 模式来强制模型输出特定格式。阅读[文档](https://lmstudio.ai/docs/advanced/structured-output)了解更多信息",
  "llm.prediction.tools/title": "Tool Use",
  "llm.prediction.tools/description": "Advanced: you can provide a JSON-compliant list of tools for the model to request calls to. Read the [documentation](https://lmstudio.ai/docs/advanced/tool-use) to learn more",
  "llm.prediction.tools/serverPageDescriptionAddon": "Pass this through the request body as `tools` when using the server API",
  "llm.prediction.promptTemplate/title": "提示模板",
  "llm.prediction.promptTemplate/subTitle": "聊天中消息发送给模型的格式。更改此设置可能会引入意外行为 - 确保您知道自己在做什么！",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Draft Tokens to Generate",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "The number of tokens to generate with the draft model per main model token. Find the sweet spot of compute vs. reward",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Drafting Probability Cutoff",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continue drafting until a token's probability falls below this threshold. Higher values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Min Draft Size",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Drafts smaller than this will be ignored by the main model. Higher values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Max Draft Size",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "Max number of tokens allowed in a draft. Ceiling if all token probs are > the cutoff. Lower values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.draftModel/title": "Draft Model",
  "llm.prediction.reasoning.parsing/title": "Reasoning Section Parsing",
  "llm.prediction.reasoning.parsing/subTitle": "How to parse reasoning sections in the model's output",

  "llm.load.contextLength/title": "上下文长度",
  "llm.load.contextLength/subTitle": "模型可以一次性关注的词元（token）最大数量。请参阅“推理参数”下的“对话溢出”选项以获取更多管理方式",
  "llm.load.contextLength/info": "指定模型一次可以考虑的最大词元（token）数量，影响其处理过程中保留的上下文量",
  "llm.load.contextLength/warning": "设置较高的上下文长度值会对内存使用产生显著影响",
  "llm.load.seed/title": "种子",
  "llm.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1 表示随机",
  "llm.load.seed/info": "随机种子：设置随机数生成的种子以确保可重复的结果",

  "llm.load.llama.evalBatchSize/title": "评估批处理大小",
  "llm.load.llama.evalBatchSize/subTitle": "每次处理的输入词元（token）数量。增加此值会提高性能，但会增加内存使用量",
  "llm.load.llama.evalBatchSize/info": "设置评估期间一起处理的示例数量，影响速度和内存使用",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 频率基",
  "llm.load.llama.ropeFrequencyBase/subTitle": "旋转位置嵌入（RoPE）的自定义基频。增加此值可能在高上下文长度下提高性能",
  "llm.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 频率比例",
  "llm.load.llama.ropeFrequencyScale/subTitle": "上下文长度按此因子缩放，以使用 RoPE 扩展有效上下文",
  "llm.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放，以控制位置编码的粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "用于 GPU 加速的离散模型层数",
  "llm.load.llama.acceleration.offloadRatio/info": "设置卸载到 GPU 的层数。",
  "llm.load.llama.flashAttention/title": "闪电注意力",
  "llm.load.llama.flashAttention/subTitle": "降低某些模型的内存使用量和生成时间",
  "llm.load.llama.flashAttention/info": "加速注意力机制，实现更快、更高效的处理",
  "llm.load.numExperts/title": "专家数量",
  "llm.load.numExperts/subTitle": "模型中使用的专家数量",
  "llm.load.numExperts/info": "模型中使用的专家数量",
  "llm.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使模型卸载到 GPU 也预留系统内存。提高性能但需要更多的系统 RAM",
  "llm.load.llama.keepModelInMemory/info": "防止模型交换到磁盘，确保更快的访问，但以更高的 RAM 使用率为代价",
  "llm.load.llama.useFp16ForKVCache/title": "使用 FP16 用于 KV 缓存",
  "llm.load.llama.useFp16ForKVCache/info": "通过以半精度（FP16）存储缓存来减少内存使用",
  "llm.load.llama.tryMmap/title": "尝试 mmap()",
  "llm.load.llama.tryMmap/subTitle": "提高模型的加载时间。禁用此功能可能在模型大于可用系统 RAM 时提高性能",
  "llm.load.llama.tryMmap/info": "直接从磁盘加载模型文件到内存",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU Thread Pool Size",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Number of CPU threads to allocate to the thread pool used for model computation",
  "llm.load.llama.cpuThreadPoolSize/info": "The number of CPU threads to allocate to the thread pool used for model computation. Increasing the number of threads does not always correlate with better performance. The default is <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "K Cache Quantization Type",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/title": "V Cache Quantization Type",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ You must disable this value if Flash Attention is not enabled",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Can only be turned on when Flash Attention is enabled",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ You must disable flash attention when using F32",
  "llm.load.mlx.kvCacheBits/title": "KV Cache Quantization",
  "llm.load.mlx.kvCacheBits/subTitle": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheBits/info": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "Context Length setting is ignored when using KV Cache Quantization",
  "llm.load.mlx.kvCacheGroupSize/title": "KV Cache Quantization: Group Size",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Group size during quantization operation for the KV cache. Higher group size reduces memory usage but may decrease quality",
  "llm.load.mlx.kvCacheGroupSize/info": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV Cache Quantization: Start quantizing when ctx crosses this length",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantization/title": "KV Cache Quantization",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantize the model's KV cache. This may result in faster generation and lower memory footprint,\nat the expense of the quality of the model output.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV cache quantization bits",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "Number of bits to quantize the KV cache to",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Group size strategy",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Accuracy",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Balanced",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "Speedy",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Advanced: Quantized 'matmul group size' configuration\n\n• Accuracy = group size 32\n• Balanced = group size 64\n• Speedy = group size 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Start quantizing when ctx reaches this length",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "When the context reaches this amount of tokens,\nbegin quantizing the KV cache",

  "embedding.load.contextLength/title": "上下文长度",
  "embedding.load.contextLength/subTitle": "模型可以一次性关注的词元（token）最大数量。请参阅“推理参数”下的“对话溢出”选项以获取更多管理方式",
  "embedding.load.contextLength/info": "指定模型一次可以考虑的最大词元（token）数量，影响其处理过程中保留的上下文量",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 频率基",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "旋转位置嵌入（RoPE）的自定义基频。增加此值可能在高上下文长度下提高性能",
  "embedding.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "embedding.load.llama.evalBatchSize/title": "评估批处理大小",
  "embedding.load.llama.evalBatchSize/subTitle": "每次处理的输入词元（token）数量。增加此值会提高性能，但会增加内存使用量",
  "embedding.load.llama.evalBatchSize/info": "设置评估期间一起处理的词元（token）数量",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 频率比例",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文长度按此因子缩放，以使用 RoPE 扩展有效上下文",
  "embedding.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放，以控制位置编码的粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "用于 GPU 加速的离散模型层数",
  "embedding.load.llama.acceleration.offloadRatio/info": "设置卸载到 GPU 的层数。",
  "embedding.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使模型卸载到 GPU 也预留系统内存。提高性能但需要更多的系统 RAM",
  "embedding.load.llama.keepModelInMemory/info": "防止模型交换到磁盘，确保更快的访问，但以更高的 RAM 使用率为代价",
  "embedding.load.llama.tryMmap/title": "尝试 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "提高模型的加载时间。禁用此功能可能在模型大于可用系统 RAM 时提高性能",
  "embedding.load.llama.tryMmap/info": "直接从磁盘加载模型文件到内存",
  "embedding.load.seed/title": "种子",
  "embedding.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1 表示随机种子",

  "embedding.load.seed/info": "随机种子：设置随机数生成的种子以确保可重复的结果",

  "presetTooltip": {
    "included/title": "预设值",
    "included/description": "以下字段将会被应用",
    "included/empty": "在此上下文中，此预设没有适用的字段。",
    "included/conflict": "您将被要求选择是否应用此值",
    "separateLoad/title": "加载时配置",
    "separateLoad/description.1": "预设还包含以下加载时配置。加载时配置是全模型范围的，并且需要重新加载模型才能生效。按住",
    "separateLoad/description.2": "应用到",
    "separateLoad/description.3": "。",
    "excluded/title": "可能不适用",
    "excluded/description": "以下字段包含在预设中，但在当前上下文中不适用。",
    "legacy/title": "旧版预设",
    "legacy/description": "这是一个旧版预设。它包括以下字段，这些字段现在要么自动处理，要么不再适用。",
    "button/publish": "Publish to Hub",
    "button/pushUpdate": "Push Changes to Hub",
    "button/export": "Export"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<空>"
    },
    "checkboxNumeric": {
      "off": "关闭"
    },
    "llamaCacheQuantizationType": {
      "off": "OFF"
    },
    "mlxKvCacheBits": {
      "off": "OFF"
    },
    "stringArray": {
      "empty": "<空>"
    },
    "llmPromptTemplate": {
      "type": "类型",
      "types.jinja/label": "模板 (Jinja)",
      "jinja.bosToken/label": "开始词元 (BOS Token)",
      "jinja.eosToken/label": "结束词元 (EOS Token)",
      "jinja.template/label": "模板",
      "jinja/error": "解析 Jinja 模板失败: {{error}}",
      "jinja/empty": "请在上方输入一个 Jinja 模板。",
      "jinja/unlikelyToWork": "您提供的 Jinja 模板很可能无法正常工作，因为它没有引用变量 \"messages\"。请检查您输入的模板是否正确。",
      "types.manual/label": "手动",
      "manual.subfield.beforeSystem/label": "系统前缀",
      "manual.subfield.beforeSystem/placeholder": "输入系统前缀...",
      "manual.subfield.afterSystem/label": "系统后缀",
      "manual.subfield.afterSystem/placeholder": "输入系统后缀...",
      "manual.subfield.beforeUser/label": "用户前缀",
      "manual.subfield.beforeUser/placeholder": "输入用户前缀...",
      "manual.subfield.afterUser/label": "用户后缀",
      "manual.subfield.afterUser/placeholder": "输入用户后缀...",
      "manual.subfield.beforeAssistant/label": "助手前缀",
      "manual.subfield.beforeAssistant/placeholder": "输入助手前缀...",
      "manual.subfield.afterAssistant/label": "助手后缀",
      "manual.subfield.afterAssistant/placeholder": "输入助手后缀...",
      "stopStrings/label": "额外停止字符串",
      "stopStrings/subTitle": "除了用户指定的停止字符串之外，还将使用特定于模板的停止字符串。"
    },
    "contextLength": {
      "maxValueTooltip": "这是模型训练所能处理的最大词元（token）数量。点击以将上下文设置为此值",
      "maxValueTextStart": "模型支持最多",
      "maxValueTextEnd": "个词元（token）",
      "tooltipHint": "尽管模型可能支持一定数量的词元（token），但如果您的机器资源无法处理负载，性能可能会下降 - 增加此值时请谨慎"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "到达限制时停止",
      "stopAtLimitSub": "一旦模型的内存满载即停止生成",
      "truncateMiddle": "截断中间",
      "truncateMiddleSub": "从对话中间移除消息以为新消息腾出空间。模型仍然会记住对话的开头",
      "rollingWindow": "滚动窗口",
      "rollingWindowSub": "模型将始终接收最近的几条消息，但可能会忘记对话的开头"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "最大",
      "off": "关闭"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "Evenly",
      "favorMainGpu": "Favor Main GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Read how it works",
      "placeholder": "Select a compatible draft model",
      "noCompatible": "No compatible draft models found for your current model selection",
      "stillLoading": "Identifying compatible draft models...",
      "notCompatible": "The selected draft model (<draft/>) is not compatible with the current model selection (<current/>).",
      "off": "OFF",
      "loadModelToSeeOptions": "Load model <keyboard-shortcut /> to see compatible options",
      "compatibleWithNumberOfModels": "Recommended for at least {{dynamicValue}} of your models",
      "recommendedForSomeModels": "Recommended for some models",
      "recommendedForLlamaModels": "Recommended for Llama models",
      "recommendedForQwenModels": "Recommended for Qwen models",
      "onboardingModal": {
        "introducing": "Introducing",
        "speculativeDecoding": "Speculative Decoding",
        "firstStepBody": "Inference speedup for <custom-span>llama.cpp</custom-span> and <custom-span>MLX</custom-span> models",
        "secondStepTitle": "Inference Speedup with Speculative Decoding",
        "secondStepBody": "Speculative Decoding is a technique involving the collaboration of two models:\n - A larger \"main\" model\n - A smaller \"draft\" model\n\nDuring generation, the draft model rapidly proposes tokens for the larger main model to verify. Verifying tokens is a much faster process than actually generating them, which is the source of the speed gains. **Generally, the larger the size difference between the main model and the draft model, the greater the speed-up**.\n\nTo maintain quality, the main model only accepts tokens that align with what it would have generated itself, enabling the response quality of the larger model at faster inference speeds. Both models must share the same vocabulary.",
        "draftModelRecommendationsTitle": "Draft model recommendations",
        "basedOnCurrentModels": "Based on your current models",
        "close": "Close",
        "next": "Next",
        "done": "Done"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Please load a model first <model-badge /> ",
      "errorEngineNotSupported": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine (<key/>) and reload the model to use this feature.",
      "errorEngineNotSupported/noKey": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine and reload the model to use this feature."
    },
    "llmReasoningParsing": {
      "startString/label": "Start String",
      "startString/placeholder": "Enter the start string...",
      "endString/label": "End String",
      "endString/placeholder": "Enter the end string..."
    }
  },
  "saveConflictResolution": {
    "title": "选择要包含在预设中的值",
    "description": "挑选并选择要保留的值",
    "instructions": "点击一个值以包含它",
    "userValues": "先前值",
    "presetValues": "新值",
    "confirm": "确认",
    "cancel": "取消"
  },
  "applyConflictResolution": {
    "title": "保留哪些值？",
    "description": "您有未提交的更改与即将应用的预设有重叠",
    "instructions": "点击一个值以保留它",
    "userValues": "当前值",
    "presetValues": "即将应用的预设值",
    "confirm": "确认",
    "cancel": "取消"
  },
  "empty": "<空>",
  "noModelSelected": "No models selected",
  "apiIdentifier.label": "API Identifier",
  "apiIdentifier.hint": "Optionally provide an identifier for this model. This will be used in API requests. Leave blank to use the default identifier.",
  "idleTTL.label": "Auto Unload If Idle (TTL)",
  "idleTTL.hint": "If set, the model will be automatically unloaded after being idle for the specified amount of time.",
  "idleTTL.mins": "mins",

  "presets": {
    "title": "预设",
    "commitChanges": "提交更改",
    "commitChanges/description": "将您的更改提交给预设。",
    "commitChanges.manual": "检测到新的字段。您将能够选择要包含在预设中的更改。",
    "commitChanges.manual.hold.0": "按住",
    "commitChanges.manual.hold.1": "选择要提交给预设的更改。",
    "commitChanges.saveAll.hold.0": "按住",
    "commitChanges.saveAll.hold.1": "保存所有更改。",
    "commitChanges.saveInPreset.hold.0": "按住",
    "commitChanges.saveInPreset.hold.1": "仅保存已经包含在预设中的字段的更改。",
    "commitChanges/error": "未能将更改提交给预设。",
    "commitChanges.manual/description": "选择要包含在预设中的更改。",
    "saveAs": "另存为新预设...",
    "presetNamePlaceholder": "为预设输入一个名称...",
    "cannotCommitChangesLegacy": "这是一个旧版预设，无法修改。您可以使用“另存为新预设...”创建一个副本。",
    "cannotCommitChangesNoChanges": "没有更改可以提交。",
    "emptyNoUnsaved": "选择一个预设...",
    "emptyWithUnsaved": "未保存的预设",
    "saveEmptyWithUnsaved": "保存预设为...",
    "saveConfirm": "保存",
    "saveCancel": "取消",
    "saving": "正在保存...",
    "save/error": "未能保存预设。",
    "deselect": "取消选择预设",
    "deselect/error": "取消选择预设失败。",
    "select/error": "选择预设失败。",
    "delete/error": "删除预设失败。",
    "discardChanges": "丢弃未保存的更改",
    "discardChanges/info": "丢弃所有未提交的更改并恢复预设至原始状态",
    "newEmptyPreset": "创建新的空预设...",
    "importPreset": "Import",
    "contextMenuSelect": "选择预设",
    "contextMenuDelete": "删除",
    "contextMenuShare": "Publish...",
    "contextMenuOpenInHub": "View on Hub",
    "contextMenuPushChanges": "Push changes to Hub",
    "contextMenuPushingChanges": "Pushing...",
    "contextMenuPushedChanges": "Changes pushed",
    "contextMenuExport": "Export File",
    "contextMenuRevealInExplorer": "Reveal in File Explorer",
    "contextMenuRevealInFinder": "Reveal in Finder",
    "share": {
      "title": "Publish Preset",
      "action": "Share your preset for others to download, like, and fork",
      "presetOwnerLabel": "Owner",
      "uploadAs": "Your preset will be created as {{name}}",
      "presetNameLabel": "Preset Name",
      "descriptionLabel": "Description (optional)",
      "loading": "Publishing...",
      "success": "Preset Successfully Pushed",
      "presetIsLive": "<preset-name /> is now live on the Hub!",
      "close": "Close",
      "confirmViewOnWeb": "View on web",
      "confirmCopy": "Copy URL",
      "confirmCopied": "Copied!",
      "pushedToHub": "Your preset was pushed to the Hub",
      "descriptionPlaceholder": "Enter a description...",
      "willBePublic": "Publishing your preset will make it public",
      "publicSubtitle": "Your preset is <custom-bold>Public</custom-bold>. Others can download and fork it on lmstudio.ai",
      "confirmShareButton": "Publish",
      "error": "Failed to publish preset",
      "createFreeAccount": "Create a free account in the Hub to publish presets"
    },
    "update": {
      "title": "Push Changes to Hub",
      "title/success": "Preset Successfully Updated",
      "subtitle": "Make changes to <custom-preset-name /> and push them to the Hub",
      "descriptionLabel": "Description",
      "descriptionPlaceholder": "Enter a description...",
      "loading": "Pushing...",
      "cancel": "Cancel",
      "createFreeAccount": "Create a free account in the Hub to publish presets",
      "error": "Failed to push update",
      "confirmUpdateButton": "Push"
    },
    "import": {
      "title": "Import a Preset from File",
      "dragPrompt": "Drag and drop preset JSON files or <custom-link>select from your computer</custom-link>",
      "remove": "Remove",
      "cancel": "Cancel",
      "importPreset_zero": "Import Preset",
      "importPreset_one": "Import Preset",
      "importPreset_other": "Import {{count}} Presets",
      "selectDialog": {
        "title": "Select Preset File (.json)",
        "button": "Import"
      },
      "error": "Failed to import preset",
      "resultsModal": {
        "titleSuccessSection_one": "Imported 1 preset successfully",
        "titleSuccessSection_other": "Imported {{count}} presets successfully",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} failed)",
        "titleFailSection_other": "({{count}} failed)",
        "titleAllFailed": "Failed to import presets",
        "importMore": "Import More",
        "close": "Done",
        "successBadge": "Success",
        "alreadyExistsBadge": "Preset already exists",
        "errorBadge": "Error",
        "invalidFileBadge": "Invalid file",
        "otherErrorBadge": "Failed to import preset",
        "errorViewDetailsButton": "View Details",
        "seeError": "See Error",
        "noName": "No preset name",
        "useInChat": "Use in Chat"
      },
      "importFromUrl": {
        "button": "Import from URL...",
        "title": "Import from URL",
        "back": "Import from File...",
        "action": "Paste the LM Studio Hub URL of the preset you want to import below",
        "invalidUrl": "Invalid URL. Please make sure you are pasting a correct LM Studio Hub URL.",
        "tip": "You can install the preset directly with the {{buttonName}} button in LM Studio Hub",
        "confirm": "Import",
        "cancel": "Cancel",
        "loading": "Importing...",
        "error": "Failed to download preset."
      }
    },
    "download": {
      "title": "Pull <preset-name /> from LM Studio Hub",
      "subtitle": "Save <custom-name /> to your presets. Doing so you will allow you to use this preset in the app",
      "button": "Pull",
      "button/loading": "Pulling...",
      "cancel": "Cancel",
      "error": "Failed to download preset."
    },
    "inclusiveness": {
      "speculativeDecoding": "Include in Preset"
    }
  },

  "flashAttentionWarning": "闪电注意力是一项实验性功能，可能会导致某些模型出现问题。如果您遇到问题，请尝试禁用它。",
  "llamaKvCacheQuantizationWarning": "KV Cache Quantization is an experimental feature that may cause issues with some models. Flash Attention must be enabled for V cache quantization. If you encounter problems, reset to the default \"F16\".",

  "seedUncheckedHint": "随机种子",
  "ropeFrequencyBaseUncheckedHint": "自动",
  "ropeFrequencyScaleUncheckedHint": "自动",

  "hardware": {
    "advancedGpuSettings": "Advanced GPU Settings",
    "advancedGpuSettings.info": "If you're unsure, leave these at their default values",
    "advancedGpuSettings.reset": "Reset to default",
    "environmentVariables": {
      "title": "Env. Variables",
      "description": "Active environment variables during model lifetime.",
      "key.placeholder": "Select var...",
      "value.placeholder": "Value"
    },
    "mainGpu": {
      "title": "Main GPU",
      "description": "The GPU to prioritize for model computation.",
      "placeholder": "Select main GPU..."
    },
    "splitStrategy": {
      "title": "Split Strategy",
      "description": "How to split model computation across GPUs.",
      "placeholder": "Select split strategy..."
    }
  }
}
